{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tab_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# initial code from https://github.com/dreamquark-ai/tabnet/blob/develop/pytorch_tabnet/tab_network.py\n",
    "import torch\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU\n",
    "import numpy as np\n",
    "import fast_tabnet.sparsemax as sparsemax\n",
    "from copy import deepcopy\n",
    "\n",
    "def initialize_non_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(4*input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "def initialize_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "class GBN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Ghost Batch Normalization\n",
    "        https://arxiv.org/abs/1705.08741\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.01):\n",
    "        super(GBN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.bn = BatchNorm1d(self.input_dim, momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(x.shape[0] // self.virtual_batch_size +\n",
    "                         ((x.shape[0] % self.virtual_batch_size) > 0))\n",
    "        res = torch.Tensor([]).to(x.device)\n",
    "        for x_ in chunks:\n",
    "            y = self.bn(x_)\n",
    "            res = torch.cat([res, y], dim=0)\n",
    "\n",
    "        return res\n",
    "\n",
    "class AttentiveTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, virtual_batch_size=128, momentum=0.02):\n",
    "        \"\"\"\n",
    "        Initialize an attention transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - input_dim : int\n",
    "            Input size\n",
    "        - output_dim : int\n",
    "            Outpu_size\n",
    "        - momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        \"\"\"\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.fc = Linear(input_dim, output_dim, bias=False)\n",
    "        initialize_non_glu(self.fc, input_dim, output_dim)\n",
    "        self.bn = GBN(output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "        # Sparsemax\n",
    "        self.sp_max = sparsemax.Sparsemax(dim=-1)\n",
    "        # Entmax\n",
    "        # self.sp_max = sparsemax.Entmax15(dim=-1)\n",
    "\n",
    "    def forward(self, priors, processed_feat):\n",
    "        x = self.fc(processed_feat)\n",
    "        x = self.bn(x)\n",
    "        x = torch.mul(x, priors)\n",
    "        x = self.sp_max(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, shared_blocks, n_glu,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(FeatTransformer, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a feature transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - input_dim : int\n",
    "            Input size\n",
    "        - output_dim : int\n",
    "            Outpu_size\n",
    "        - shared_blocks : torch.nn.Module\n",
    "            The shared block that should be common to every step\n",
    "        - momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        \"\"\"\n",
    "\n",
    "        self.shared = deepcopy(shared_blocks)\n",
    "        if self.shared is not None:\n",
    "            for l in self.shared.glu_layers:\n",
    "                l.bn = GBN(2*output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                           momentum=momentum)\n",
    "\n",
    "        if self.shared is None:\n",
    "            self.specifics = GLU_Block(input_dim, output_dim,\n",
    "                                       n_glu=n_glu,\n",
    "                                       first=True,\n",
    "                                       virtual_batch_size=virtual_batch_size,\n",
    "                                       momentum=momentum)\n",
    "        else:\n",
    "            self.specifics = GLU_Block(output_dim, output_dim,\n",
    "                                       n_glu=n_glu,\n",
    "                                       virtual_batch_size=virtual_batch_size,\n",
    "                                       momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shared is not None:\n",
    "            x = self.shared(x)\n",
    "        x = self.specifics(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Independant GLU block, specific to each step\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, n_glu=2, first=False,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Block, self).__init__()\n",
    "        self.first = first\n",
    "        self.n_glu = n_glu\n",
    "        self.glu_layers = torch.nn.ModuleList()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5]))\n",
    "        for glu_id in range(self.n_glu):\n",
    "            if glu_id == 0:\n",
    "                self.glu_layers.append(GLU_Layer(input_dim, output_dim,\n",
    "                                                 virtual_batch_size=virtual_batch_size,\n",
    "                                                 momentum=momentum))\n",
    "            else:\n",
    "                self.glu_layers.append(GLU_Layer(output_dim, output_dim,\n",
    "                                                 virtual_batch_size=virtual_batch_size,\n",
    "                                                 momentum=momentum))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.scale = self.scale.to(x.device)\n",
    "        if self.first:  # the first layer of the block has no scale multiplication\n",
    "            x = self.glu_layers[0](x)\n",
    "            layers_left = range(1, self.n_glu)\n",
    "        else:\n",
    "            layers_left = range(self.n_glu)\n",
    "\n",
    "        for glu_id in layers_left:\n",
    "            x = torch.add(x, self.glu_layers[glu_id](x))\n",
    "            x = x*self.scale\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Layer, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.fc = Linear(input_dim, 2*output_dim, bias=False)\n",
    "        initialize_glu(self.fc, input_dim, 2*output_dim)\n",
    "\n",
    "        self.bn = GBN(2*output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        out = torch.mul(x[:, :self.output_dim], torch.sigmoid(x[:, self.output_dim:]))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai2]",
   "language": "python",
   "name": "conda-env-fastai2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
